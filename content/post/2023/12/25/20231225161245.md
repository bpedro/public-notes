---
date: 2023-12-25T16:12:45+01:00
link: https://arxiv.org/abs/1910.13461
tags:
  - ai
  - summarization
---
{{% marginnote %}}Found at "[[1910.13461] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://web.archive.org/web/20231225161245/https://arxiv.org/abs/1910.13461)" on 2023-12-25 16:12:45 +01:00.{{% /marginnote %}}

> BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture